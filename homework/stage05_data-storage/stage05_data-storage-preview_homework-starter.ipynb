{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c0b379-c3a5-4893-be63-68c362eabc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directories exist: data/raw and data/processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get paths from .env file\n",
    "DATA_DIR_RAW = os.getenv('DATA_DIR_RAW')\n",
    "DATA_DIR_PROCESSED = os.getenv('DATA_DIR_PROCESSED')\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "os.makedirs(DATA_DIR_RAW, exist_ok=True)\n",
    "os.makedirs(DATA_DIR_PROCESSED, exist_ok=True)\n",
    "\n",
    "print(f\"Ensured directories exist: {DATA_DIR_RAW} and {DATA_DIR_PROCESSED}\")\n",
    "\n",
    "# Now your original code can run without the error\n",
    "# ...\n",
    "# df.to_csv(os.path.join(DATA_DIR_RAW, 'sample_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089e39ed-b5ac-4b03-98e3-215bea7bb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory: data/raw\n",
      "Processed data directory: data/processed\n",
      "DataFrame saved to CSV at: data/raw\\sample_data.csv\n",
      "DataFrame saved to Parquet at: data/processed\\sample_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Save in Two Formats\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get paths from environment variables\n",
    "DATA_DIR_RAW = os.getenv('DATA_DIR_RAW')\n",
    "DATA_DIR_PROCESSED = os.getenv('DATA_DIR_PROCESSED')\n",
    "\n",
    "print(f\"Raw data directory: {DATA_DIR_RAW}\")\n",
    "print(f\"Processed data directory: {DATA_DIR_PROCESSED}\")\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'value': [10.5, 20.2, 30.7, 40.1],\n",
    "    'is_active': [True, False, True, False]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV in the raw directory\n",
    "csv_path = os.path.join(DATA_DIR_RAW, 'sample_data.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"DataFrame saved to CSV at: {csv_path}\")\n",
    "\n",
    "# Save to Parquet in the processed directory\n",
    "# You may need to run: pip install pyarrow\n",
    "parquet_path = os.path.join(DATA_DIR_PROCESSED, 'sample_data.parquet')\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "print(f\"DataFrame saved to Parquet at: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74636d1e-0143-43b0-9de2-975ef6a05832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation ---\n",
      "\n",
      "Validating CSV format...\n",
      "Shapes match: True\n",
      "Dtypes match: True\n",
      "Original dtypes:\n",
      " id             int64\n",
      "name          object\n",
      "value        float64\n",
      "is_active       bool\n",
      "dtype: object\n",
      "Reloaded dtypes:\n",
      " id             int64\n",
      "name          object\n",
      "value        float64\n",
      "is_active       bool\n",
      "dtype: object\n",
      "✅ Validation successful for CSV!\n",
      "\n",
      "Validating Parquet format...\n",
      "Shapes match: True\n",
      "Dtypes match: True\n",
      "Original dtypes:\n",
      " id             int64\n",
      "name          object\n",
      "value        float64\n",
      "is_active       bool\n",
      "dtype: object\n",
      "Reloaded dtypes:\n",
      " id             int64\n",
      "name          object\n",
      "value        float64\n",
      "is_active       bool\n",
      "dtype: object\n",
      "✅ Validation successful for Parquet!\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Reload and Validate\n",
    "\n",
    "# Reload the CSV and Parquet files\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "df_parquet = pd.read_parquet(parquet_path)\n",
    "\n",
    "print(\"\\n--- Validation ---\")\n",
    "\n",
    "# Define a validation function\n",
    "def validate_dataframes(original_df, reloaded_df, format_name):\n",
    "    \"\"\"\n",
    "    Compares original and reloaded DataFrames to check for consistency.\n",
    "    \"\"\"\n",
    "    print(f\"\\nValidating {format_name} format...\")\n",
    "    # Check shapes\n",
    "    shapes_match = original_df.shape == reloaded_df.shape\n",
    "    print(f\"Shapes match: {shapes_match}\")\n",
    "\n",
    "    # Check dtypes for critical columns\n",
    "    dtypes_match = all(original_df.dtypes == reloaded_df.dtypes)\n",
    "    # Note: CSV may load 'is_active' as object or bool, while parquet preserves it correctly\n",
    "    print(f\"Dtypes match: {dtypes_match}\")\n",
    "    print(\"Original dtypes:\\n\", original_df.dtypes)\n",
    "    print(\"Reloaded dtypes:\\n\", reloaded_df.dtypes)\n",
    "\n",
    "    if shapes_match and dtypes_match:\n",
    "        print(f\"✅ Validation successful for {format_name}!\")\n",
    "    else:\n",
    "        print(f\"❌ Validation failed for {format_name}.\")\n",
    "\n",
    "# Validate the reloaded DataFrames\n",
    "validate_dataframes(df, df_csv, \"CSV\")\n",
    "validate_dataframes(df, df_parquet, \"Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c2e510-0a2b-48df-991a-809c3dc0b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written to Parquet: data/processed\\new_data.parquet\n",
      "DataFrame successfully written to CSV: data/raw\\new_data.csv\n",
      "\n",
      "Reloaded DataFrame from Parquet:\n",
      "    a  b\n",
      "0  1  3\n",
      "1  2  4\n",
      "\n",
      "Reloaded DataFrame from CSV:\n",
      "    a  b\n",
      "0  1  3\n",
      "1  2  4\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Refactor to Utilities\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def write_df(df: pd.DataFrame, filepath: str):\n",
    "    \"\"\"Writes a DataFrame to a file based on its suffix (.csv, .parquet).\"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "    # Determine file format from suffix\n",
    "    if filepath.endswith('.csv'):\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"DataFrame successfully written to CSV: {filepath}\")\n",
    "    elif filepath.endswith('.parquet'):\n",
    "        try:\n",
    "            df.to_parquet(filepath, index=False)\n",
    "            print(f\"DataFrame successfully written to Parquet: {filepath}\")\n",
    "        except ImportError:\n",
    "            warnings.warn(\"Parquet engine (e.g., 'pyarrow') not found. Please install it to write Parquet files.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
    "\n",
    "def read_df(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads a DataFrame from a file based on its suffix (.csv, .parquet).\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    if filepath.endswith('.csv'):\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filepath.endswith('.parquet'):\n",
    "        return pd.read_parquet(filepath)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
    "\n",
    "# Example usage of the utility functions\n",
    "new_df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n",
    "\n",
    "# Write using the utility function\n",
    "write_df(new_df, os.path.join(DATA_DIR_PROCESSED, 'new_data.parquet'))\n",
    "write_df(new_df, os.path.join(DATA_DIR_RAW, 'new_data.csv'))\n",
    "\n",
    "# Read using the utility function\n",
    "reloaded_parquet_df = read_df(os.path.join(DATA_DIR_PROCESSED, 'new_data.parquet'))\n",
    "reloaded_csv_df = read_df(os.path.join(DATA_DIR_RAW, 'new_data.csv'))\n",
    "\n",
    "print(\"\\nReloaded DataFrame from Parquet:\\n\", reloaded_parquet_df)\n",
    "print(\"\\nReloaded DataFrame from CSV:\\n\", reloaded_csv_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
